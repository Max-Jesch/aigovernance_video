{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 1. Setup AI Fact Sheets"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "get the api key from your ibm-cloud interface. \n\napi_key = 'xxx'"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "insert the project token by clicking \"insert project token\" behind the three dots in the top right corner. You might have to create process token first in the cp4d/project/manage/access_control interface"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "import os\n\ntry:\n    from ibm_aigov_facts_client import AIGovFactsClient\nexcept:\n    !pip install -U ibm-aigov-facts-client\n    from ibm_aigov_facts_client import AIGovFactsClient\n        \n        \nPROJECT_UID= os.environ['PROJECT_ID'] #this assumes you are running in Studio. When running externally please add the project id here.\nCPD_URL=os.environ['RUNTIME_ENV_APSX_URL'][len('https://api.'):] #the variable starts with https://api.\" and you only need the rest 'dataplatform.dev.cloud.ibm.com'\nCONTAINER_ID=PROJECT_UID\nCONTAINER_TYPE='project'\nEXPERIMENT_NAME='car_insurance_outcome_prediction'\n# CPD4D API key. Navigate to hamburger menu -> Acess (IAM) -> API Keys to create one for you (or have your account admin do it)\n\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nPROJECT_ACCESS_TOKEN=project.project_context.accessToken.replace('Bearer ','') #You can create or lookup such a token from the settings tab of this project\n\n\n\nfacts_client = AIGovFactsClient(api_key=api_key,experiment_name=EXPERIMENT_NAME,container_type=CONTAINER_TYPE,container_id=CONTAINER_ID,set_as_current_experiment=True)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 2. Train Model\n\n### using Pipeline 4 Notebook - AutoAI Notebook v1.17.1\n![image](https://github.com/IBM/watson-machine-learning-samples/raw/master/cloud/notebooks/headers/AutoAI-Banner_Pipeline-Notebook.png)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"install\"></a>\n### Package installation\nBefore you use the sample code in this notebook, install the following packages:\n - ibm_watson_machine_learning,\n - autoai-libs,\n - scikit-learn\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2020-10-12T14:00:45.009458Z",
                    "iopub.status.busy": "2020-10-12T14:00:45.007968Z",
                    "iopub.status.idle": "2020-10-12T14:00:46.037702Z",
                    "shell.execute_reply": "2020-10-12T14:00:46.038270Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                },
                "scrolled": true
            },
            "outputs": [],
            "source": "!pip install ibm-watson-machine-learning | tail -n 1\n!pip install autoai-libs==1.14.9 | tail -n 1\n!pip install scikit-learn==1.1.1 | tail -n 1"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"variables_definition\"></a>\n### AutoAI experiment metadata \nThe following cell contains the training data connection details.  \n**Note**: The connection might contain authorization credentials, so be careful when sharing the notebook."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "copy this part from the auto AI notebook\n```\ntraining_data_references = [\n    DataConnection(\n        data_asset_id='de66f9ac-2117-4f39-bf15-ba4cee99a4bc'\n    ),\n]\ntraining_result_reference = DataConnection(\n    location=ContainerLocation(\n        path='xxx',\n        model_location='xxx',\n        training_status='xxx'\n    )\n)\n```"
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2020-10-12T14:00:49.797633Z",
                    "iopub.status.busy": "2020-10-12T14:00:49.796778Z",
                    "iopub.status.idle": "2020-10-12T14:00:57.182715Z",
                    "shell.execute_reply": "2020-10-12T14:00:57.183132Z"
                },
                "pycharm": {
                    "is_executing": true
                }
            },
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The following cell contains input parameters provided to run the AutoAI experiment in Watson Studio."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2020-10-12T14:00:57.187305Z",
                    "iopub.status.busy": "2020-10-12T14:00:57.186602Z",
                    "iopub.status.idle": "2020-10-12T14:00:57.188392Z",
                    "shell.execute_reply": "2020-10-12T14:00:57.188878Z"
                },
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": "experiment_metadata = dict(\n    prediction_type='binary',\n    prediction_column='OUTCOME',\n    holdout_size=0.1,\n    scoring='accuracy',\n    csv_separator=',',\n    random_state=33,\n    max_number_of_estimators=2,\n    training_data_references=training_data_references,\n    training_result_reference=training_result_reference,\n    deployment_url='https://us-south.ml.cloud.ibm.com',\n    project_id='c7354a00-b66b-4896-8443-40b1ac51c846',\n    positive_label='0.0',\n    drop_duplicates=True\n)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"connection\"></a>\n## Watson Machine Learning connection\n\nThis cell defines the credentials required to work with the Watson Machine Learning service.\n\n**Action**: Provide the IBM Cloud apikey, For details, see [documentation](https://cloud.ibm.com/docs/account?topic=account-userapikey)."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "wml_credentials = {\n    \"apikey\": api_key,\n    \"url\": experiment_metadata['deployment_url']\n}"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from ibm_watson_machine_learning import APIClient\n\nwml_client = APIClient(wml_credentials)\n\nif 'space_id' in experiment_metadata:\n    wml_client.set.default_space(experiment_metadata['space_id'])\nelse:\n    wml_client.set.default_project(experiment_metadata['project_id'])\n    \ntraining_data_references[0].set_client(wml_client)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Read and preprocess data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2020-10-12T14:01:16.076169Z",
                    "iopub.status.busy": "2020-10-12T14:01:16.075589Z",
                    "iopub.status.idle": "2020-10-12T14:01:19.190233Z",
                    "shell.execute_reply": "2020-10-12T14:01:19.190807Z"
                },
                "pycharm": {
                    "is_executing": true,
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": "train_X, test_X, train_y, test_y = training_data_references[0].read(experiment_metadata=experiment_metadata, with_holdout_split=True, use_flight=False)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": "<a id=\"preview_model_to_python_code\"></a>\n## Create pipeline\nIn the next cell, you can find the Scikit-learn definition of the selected AutoAI pipeline."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": "#### Import statements."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "pycharm": {
                    "is_executing": true,
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": "from autoai_libs.transformers.exportable import ColumnSelector\nfrom autoai_libs.transformers.exportable import NumpyColumnSelector\nfrom autoai_libs.transformers.exportable import CompressStrings\nfrom autoai_libs.transformers.exportable import NumpyReplaceMissingValues\nfrom autoai_libs.transformers.exportable import NumpyReplaceUnknownValues\nfrom autoai_libs.transformers.exportable import boolean2float\nfrom autoai_libs.transformers.exportable import CatImputer\nfrom autoai_libs.transformers.exportable import CatEncoder\nimport numpy as np\nfrom autoai_libs.transformers.exportable import float32_transform\nfrom sklearn.pipeline import make_pipeline\nfrom autoai_libs.transformers.exportable import FloatStr2Float\nfrom autoai_libs.transformers.exportable import NumImputer\nfrom autoai_libs.transformers.exportable import OptStandardScaler\nfrom sklearn.pipeline import make_union\nfrom autoai_libs.transformers.exportable import NumpyPermuteArray\nfrom autoai_libs.cognito.transforms.transform_utils import TNoOp\nfrom sklearn.ensemble import RandomForestClassifier"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Pre-processing & Estimator."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "pycharm": {
                    "is_executing": true,
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": "column_selector_0 = ColumnSelector(\n    columns_indices_list=[\n        1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n    ]\n)\nnumpy_column_selector_0 = NumpyColumnSelector(\n    columns=[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n)\ncompress_strings = CompressStrings(\n    compress_type=\"hash\",\n    dtypes_list=[\n        \"char_str\", \"char_str\", \"char_str\", \"char_str\", \"char_str\",\n        \"char_str\", \"float_int_num\", \"char_str\", \"float_int_num\",\n        \"float_int_num\", \"int_num\", \"float_int_num\", \"char_str\", \"int_num\",\n        \"int_num\", \"int_num\",\n    ],\n    missing_values_reference_list=[\"\", \"-\", \"?\", float(\"nan\")],\n    misslist_list=[\n        [], [], [], [], [], [], [], [], [], [], [],\n        [float(\"nan\")],\n        [], [], [], [],\n    ],\n)\nnumpy_replace_missing_values_0 = NumpyReplaceMissingValues(\n    missing_values=[float(\"nan\")], filling_values=float(\"nan\")\n)\nnumpy_replace_unknown_values = NumpyReplaceUnknownValues(\n    filling_values=float(\"nan\"),\n    filling_values_list=[\n        float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"),\n        float(\"nan\"), 100001, float(\"nan\"), 100001, 100001, float(\"nan\"),\n        100001, float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"),\n    ],\n    missing_values_reference_list=[\"\", \"-\", \"?\", float(\"nan\")],\n)\ncat_imputer = CatImputer(\n    missing_values=float(\"nan\"),\n    sklearn_version_family=\"1\",\n    strategy=\"most_frequent\",\n)\ncat_encoder = CatEncoder(\n    encoding=\"ordinal\",\n    categories=\"auto\",\n    dtype=np.float64,\n    handle_unknown=\"error\",\n    sklearn_version_family=\"1\",\n)\npipeline_0 = make_pipeline(\n    column_selector_0,\n    numpy_column_selector_0,\n    compress_strings,\n    numpy_replace_missing_values_0,\n    numpy_replace_unknown_values,\n    boolean2float(),\n    cat_imputer,\n    cat_encoder,\n    float32_transform(),\n)\ncolumn_selector_1 = ColumnSelector(\n    columns_indices_list=[\n        1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n    ]\n)\nnumpy_column_selector_1 = NumpyColumnSelector(columns=[6])\nfloat_str2_float = FloatStr2Float(\n    dtypes_list=[\"float_num\"], missing_values_reference_list=[float(\"nan\")]\n)\nnumpy_replace_missing_values_1 = NumpyReplaceMissingValues(\n    missing_values=[float(\"nan\")], filling_values=float(\"nan\")\n)\nnum_imputer = NumImputer(missing_values=float(\"nan\"), strategy=\"median\")\nopt_standard_scaler = OptStandardScaler(use_scaler_flag=False)\npipeline_1 = make_pipeline(\n    column_selector_1,\n    numpy_column_selector_1,\n    float_str2_float,\n    numpy_replace_missing_values_1,\n    num_imputer,\n    opt_standard_scaler,\n    float32_transform(),\n)\nunion = make_union(pipeline_0, pipeline_1)\nnumpy_permute_array = NumpyPermuteArray(\n    axis=0,\n    permutation_indices=[\n        0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 6,\n    ],\n)\nt_no_op = TNoOp(\n    fun=None, name=\"no_action\", datatypes=\"x\", feat_constraints=[]\n)\nrandom_forest_classifier = RandomForestClassifier(\n    class_weight=\"balanced\",\n    criterion=\"entropy\",\n    max_depth=5,\n    max_features=0.9938750513379689,\n    min_samples_leaf=5,\n    n_estimators=97,\n    n_jobs=4,\n    random_state=33,\n)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Pipeline."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "pycharm": {
                    "is_executing": true,
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": "pipeline = make_pipeline(\n    union, numpy_permute_array, t_no_op, random_forest_classifier\n)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"train\"></a>\n## Train pipeline model\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": "### Define scorer from the optimization metric\nThis cell constructs the cell scorer based on the experiment metadata."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "pycharm": {
                    "is_executing": true,
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": "from sklearn.metrics import get_scorer\n\nscorer = get_scorer(experiment_metadata['scoring'])"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": "<a id=\"test_model\"></a>\n### Fit pipeline model\nIn this cell, the pipeline is fitted."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2020-10-12T14:01:19.291734Z",
                    "iopub.status.busy": "2020-10-12T14:01:19.244735Z",
                    "iopub.status.idle": "2020-10-12T14:01:19.338461Z",
                    "shell.execute_reply": "2020-10-12T14:01:19.338958Z"
                },
                "pycharm": {
                    "is_executing": true,
                    "name": "#%%\n"
                },
                "scrolled": true
            },
            "outputs": [],
            "source": "pipeline.fit(train_X.values, train_y.values.ravel());"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"test_model\"></a>\n## Test pipeline model"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": "Score the fitted pipeline with the generated scorer using the holdout dataset."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2020-10-12T14:02:03.910267Z",
                    "iopub.status.busy": "2020-10-12T14:02:03.909710Z",
                    "iopub.status.idle": "2020-10-12T14:02:03.914154Z",
                    "shell.execute_reply": "2020-10-12T14:02:03.914727Z"
                },
                "pycharm": {
                    "is_executing": true,
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": "score = scorer(pipeline, test_X.values, test_y.values)\nprint(score)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": "pipeline.predict(test_X.values[:10])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 3. Save model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "model_name = 'ai_governance_test_model'"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "fields=train_X.columns.tolist()\ntarget_col = train_y.columns.tolist()[0]\nmetadata_dict = {'target_col' : target_col,'fields':fields}"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# TODO: \n# adapt the \"saving model\" from the original \"1-model-training-with-factsheets\" to this model\n# https://dataplatform.cloud.ibm.com/analytics/notebooks/v2/fd15bd20-0d9c-45b2-9b25-50312bab7084/view?projectid=c7354a00-b66b-4896-8443-40b1ac51c846&context=cpdaas\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "collect a bunch of informations and hand them over to the facts_client"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "fields=train_X.columns.tolist()\ntarget_col = train_y.columns.tolist()[0]\nmetadata_dict = {'target_col' : target_col,'fields':fields}\n\n\ntraining_data_references = [\n                {\n                    \"id\": \"car_insurance_data\",\n                    \"type\": \"connection_asset\",\n                    \"location\": {\n                        \"select_statement\": \"training_data_references[0].read(experiment_metadata=experiment_metadata, with_holdout_split=True, use_flight=False)\",\n                        \"table_name\": \"car_insurance_data imported from AutoAI\"\n                    }\n                }]\n\nsoftware_spec_uid = wml_client.software_specifications.get_id_by_name(\"runtime-22.1-py3.9\")\n\n\nmodel_props = {\n        wml_client._models.ConfigurationMetaNames.NAME:model_name,\n        wml_client._models.ConfigurationMetaNames.TYPE: \"scikit-learn_1.0\",\n        wml_client._models.ConfigurationMetaNames.SOFTWARE_SPEC_UID: software_spec_uid,\n        wml_client._models.ConfigurationMetaNames.LABEL_FIELD:\"car_insurance\",\n        wml_client._models.ConfigurationMetaNames.TAGS: ['car_insurance_outcome_v01'],\n        wml_client.repository.ModelMetaNames.TRAINING_DATA_REFERENCES: training_data_references,\n        wml_client._models.ConfigurationMetaNames.CUSTOM: metadata_dict\n    }\n\nfacts_client.export_facts.prepare_model_meta(wml_client=wml_client,meta_props=model_props)\n\n#Defining Notebook URL\nnb_name = \"1-model-training-with-factsheet\"\nnb_asset_id = \"tbd\"\nnb_asset_url = \"https://\" + CPD_URL + \"/analytics/notebooks/v2/\" + nb_asset_id + \"?projectid=\" + PROJECT_UID + \"&context=cpdaas\"\n\nlatestRunId = facts_client.runs.list_runs_by_experiment('1').sort_values('start_time').iloc[-1]['run_id']\nfacts_client.runs.set_tags(latestRunId, {\"Notebook name\": nb_name, \"Notebook id\": nb_asset_id, \"Notebook URL\" : nb_asset_url})\nfacts_client.export_facts.export_payload(latestRunId)\n\nRUN_ID=facts_client.runs.get_current_run_id()\nfacts_client.export_facts.export_payload(RUN_ID)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(\"Storing model .....\")\n\npublished_model_details = wml_client.repository.store_model(model=pipeline, \n                                                        meta_props=model_props, \n                                                        training_data=train_X, \n                                                        training_target=train_y)\nmodel_uid = wml_client.repository.get_model_id(published_model_details)\nprint(\"The model\",model_name,\"successfully stored in the project\")\nprint(\"Model ID: {}\".format(model_uid))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}